{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - from TCR Data to Prompts to Analysis Results\n",
    "\n",
    "---\n",
    "\n",
    "This scrip helps reproduce the results in the TCR paper by performing the following steps:\n",
    "* Create Prompts\n",
    "* (Not included) Run the Prompts\n",
    "* Parse the responses\n",
    "* Analyze the results as a binary classification problem and focus on the \"not discussed\" class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.benchmark_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Prompts\n",
    "\n",
    "In this section, we convert the TCR data to prompts with predefined prompt template by different snippet length. Except for the prompt template, the settings are the same as the benchmark experiments in the TCR paper.\n",
    "\n",
    "* [**REQUIRED**] To update the prompt template, please overwrite the `prompt_topic_relevance` function by updating the cell below. Currently it is just a placeholder without actual prompt requests.\n",
    "\n",
    "* To change other settings, please modify cuntions in `benchmark_utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================================\n",
    "# Overwrite the following functions with your own implementation\n",
    "# ========================================================================================================\n",
    "\n",
    "def prompt_topic_relevance(str_trans, str_topics, len_topics, quotes_style=''):\n",
    "    '''\n",
    "    Generate a prompt based on the template and inputs to determine the relevance of topics in a conversation transcript.\n",
    "    Parameters:\n",
    "        str_trans (str): The conversation transcript.\n",
    "        str_topics (str): The list of topics being discussed.\n",
    "        len_topics (int): The length of the topic list.\n",
    "        quotes_style (str): Optional. Specifies the style of quotes to unify. Valid values are \"single\", \"double\", or an empty string. Default is an empty string.\n",
    "    Returns:\n",
    "        str: The generated prompt.\n",
    "    '''\n",
    "    prompt_template = '''HERE IS THE PROMPT TEMPATE THAT TAKES THE FOLLOWING INPUTS: {input_trans}, {input_topics}, {len_topics}. '''\n",
    "    prompt_template = prompt_template.replace('\\n    ', '\\n')\n",
    "    str_trans = unify_quotes(str_trans, style=quotes_style)\n",
    "    str_prompt = prompt_template.format(input_trans=str_trans, input_topics=str_topics, len_topics=len_topics)\n",
    "    return str_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs (Folder that inlcudes the json files in the TCR data format)\n",
    "input_folder = 'data\\example_aug'\n",
    "\n",
    "# Outputs (Folder to save the generated prompts in json format)\n",
    "output_folder = 'tmp\\prompts'\n",
    "prep_folder(output_folder)\n",
    "    \n",
    "# Generate prompts\n",
    "t_pfiles = [x for x in locate_suffix_files(input_folder, suffix='json')]\n",
    "list_prompt = generate_relevance_prompts(t_pfiles)\n",
    "\n",
    "# Write to json: one json per Line\n",
    "prompt_path = write_prompts_to_json(list_prompt, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Prompts \n",
    "\n",
    "Run the prompts generated above by using your selected model. \n",
    "\n",
    "We assume all responses are stored in a single JSON file with the `prompId: response` key-value paris. In the section below, we refer to this file path as `result_path`.\n",
    "\n",
    "An example of the results in the `result_path` file\n",
    "```\n",
    "{\n",
    "    \"ICSI_aug_addTopics_w_topics_Bed004_variation_addToics_0_300_5\": \"THIS IS A RANDOM LIST [1, 3, 2, 0]\",\n",
    "    \"ICSI_aug_addTopics_w_topics_Bed009_variation_addToics_0_300_7\": \"THIS IS A RANDOM LIST [0, 0, 2, 1]\",\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Results\n",
    "\n",
    "Depending on your prompts, the returned responses may have a different format. Overwrite the function `prompt_styles` to reflect how your responses are structured.\n",
    "\n",
    "In the process below, we assume that the response contains a list of scores that has the same length as the input topic list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================================\n",
    "# Overwrite the following functions with your own implementation\n",
    "# ========================================================================================================\n",
    "\n",
    "def prompt_styles():\n",
    "    '''\n",
    "    Define the response parts and style examples for parsing responses.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the response parts as keys and their types as values.\n",
    "        list or str: The style example(s) to be replaced in the responses.\n",
    "    '''\n",
    "    response_parts = {\n",
    "        'THIS IS A RANDOM LIST': 'list',\n",
    "    }\n",
    "    list_to_remove = ['']\n",
    "    return response_parts, list_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground truth data\n",
    "df_gt = get_gt_data(prompt_path)\n",
    "fname = os.path.split(prompt_path)[-1].replace('.json', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Topic-Conversation Relevance results\n",
    "result_path = 'PATH_TO_YOUR_RESULTS_JSON_FILE'\n",
    "response_parts, list_to_remove = prompt_styles()\n",
    "all_responses = get_raw_completions(result_path)\n",
    "df_model, error_contents = parse_responses(all_responses, df_gt, list_to_remove, response_parts)\n",
    "if len(error_contents) > 0:\n",
    "    raise ValueError(f'To continue, handle the following error contents: {error_contents}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the results\n",
    "df_all_join, list_buckets, threshold_pairs = join_results(df_gt, df_model, fname, output_folder)\n",
    "df_all_join['prompt_size'] = df_all_join['promptId'].str.split('_').str[-2]\n",
    "for psg, psdf in df_all_join.groupby('prompt_size'):\n",
    "    for b in list_buckets:\n",
    "        for p in threshold_pairs[b]:\n",
    "            ptitle = ' '.join(['\\n', '='*10, 'Evaluation Results for {0} snippets; classification {1}; threshold {2}'.format(psg, b, p), '='*10])\n",
    "            metrics, metrics_binary = calculate_metrics(psdf, b, p)\n",
    "            display_metrics(metrics, metrics_binary, title=ptitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
